{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9ulOfg6EWON"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PeaFMs4ceL4m"
      },
      "outputs": [],
      "source": [
        "!pip install -q ml_collections\n",
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WALwFWNcek5S"
      },
      "outputs": [],
      "source": [
        "# Backbone\n",
        "!wget https://dl.fbaipublicfiles.com/dino/dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth -q\n",
        "\n",
        "# Linear layer\n",
        "!wget https://dl.fbaipublicfiles.com/dino/dino_vitbase16_pretrain/dino_vitbase16_linearweights.pth -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "URcMpFkFetMW"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "giYjHQXTe3ri"
      },
      "outputs": [],
      "source": [
        "backbone_state_dict = torch.load(\n",
        "    \"dino_vitbase16_pretrain.pth\", map_location=torch.device(\"cpu\")\n",
        ")\n",
        "linear_layer_state_dict = torch.load(\n",
        "    \"dino_vitbase16_linearweights.pth\", map_location=torch.device(\"cpu\")\n",
        ")[\"state_dict\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gxUU6kI-e33s"
      },
      "outputs": [],
      "source": [
        "backbone_state_dict.update(linear_layer_state_dict)\n",
        "backbone_state_dict[\"head.weight\"] = backbone_state_dict.pop(\"module.linear.weight\")\n",
        "backbone_state_dict[\"head.bias\"] = backbone_state_dict.pop(\"module.linear.bias\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjxPufX6e5vn",
        "outputId": "456e7b6b-8004-4ce5-ae9d-c8c8700682e6"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/abhilashreddys/Explaining-Vision-Transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUWrvhf5EZbj"
      },
      "source": [
        "## Explaining Vision Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QDiIkb8ce7ui"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# sys.path.append(\"Explaining-Vision-Transformers\")\n",
        "\n",
        "from vit.vit_models import ViTClassifier\n",
        "from vit.model_configs import base_config\n",
        "from utils import helpers\n",
        "from vit.layers import mha\n",
        "\n",
        "from transformers.tf_utils import shape_list\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import ml_collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jyFksKPse-jN"
      },
      "outputs": [],
      "source": [
        "class ViTDINOBase(ViTClassifier):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def interpolate_pos(self, x, N, h, w):\n",
        "        class_pos_embed = self.positional_embedding[:, 0]\n",
        "        patch_pos_embed = self.positional_embedding[:, 1:]\n",
        "        dim = shape_list(x)[-1]\n",
        "\n",
        "        # Calculate the resolution to which we need to perform interpolation.\n",
        "        h0 = h // self.config.patch_size\n",
        "        w0 = w // self.config.patch_size\n",
        "\n",
        "        sqrt_N = tf.math.sqrt(tf.cast(N, \"float32\"))\n",
        "        sqrt_N_ceil = tf.cast(tf.math.ceil(sqrt_N), \"int32\")\n",
        "        patch_pos_embed = tf.reshape(\n",
        "            patch_pos_embed, (1, sqrt_N_ceil, sqrt_N_ceil, dim)\n",
        "        )\n",
        "        patch_pos_embed = tf.image.resize(patch_pos_embed, (h0, w0), method=\"bicubic\")\n",
        "\n",
        "        tf.debugging.assert_equal(h0, shape_list(patch_pos_embed)[1])\n",
        "        tf.debugging.assert_equal(w0, shape_list(patch_pos_embed)[2])\n",
        "\n",
        "        patch_pos_embed = tf.reshape(patch_pos_embed, (1, -1, dim))\n",
        "        return tf.concat([class_pos_embed[None, ...], patch_pos_embed], axis=1)\n",
        "\n",
        "    def interpolate_pos_embedding(self, x, h, w):\n",
        "        \"\"\"Resizes the positional embedding in case there is a mismatch in resolution.\n",
        "        E.g., using 480x480 images instead of 224x224 for a given patch size.\n",
        "        \"\"\"\n",
        "        num_patches = shape_list(x)[1] - 1  # Exlcuding the cls token.\n",
        "        N = shape_list(self.positional_embedding)[1] - 1\n",
        "\n",
        "        # Segregate the cls embedding from the position embeddings.\n",
        "        class_pos_embed = self.positional_embedding[:, 0]\n",
        "        patch_pos_embed = self.positional_embedding[:, 1:]\n",
        "        dim = shape_list(x)[-1]\n",
        "\n",
        "        pos_embed = tf.cond(\n",
        "            tf.logical_and(tf.equal(num_patches, N), tf.equal(h, w)),\n",
        "            lambda: self.positional_embedding,\n",
        "            lambda: self.interpolate_pos(x, N, h, w),\n",
        "        )\n",
        "        return pos_embed\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        n, h, w, c = shape_list(inputs)\n",
        "        projected_patches = self.projection(inputs)\n",
        "\n",
        "        cls_token = tf.tile(self.cls_token, (n, 1, 1))\n",
        "        if cls_token.dtype != projected_patches.dtype:\n",
        "            cls_token = tf.cast(cls_token, projected_patches.dtype)\n",
        "        projected_patches = tf.concat([cls_token, projected_patches], axis=1)\n",
        "\n",
        "        # Fetch positional embeddings.\n",
        "        positional_embedding = self.interpolate_pos_embedding(projected_patches, h, w)\n",
        "\n",
        "        # Add positional embeddings to the projected patches.\n",
        "        encoded_patches = (\n",
        "            positional_embedding + projected_patches\n",
        "        )  # (B, number_patches, projection_dim)\n",
        "        encoded_patches = self.dropout(encoded_patches)\n",
        "\n",
        "        # Initialize a dictionary to store attention scores from each transformer\n",
        "        # block.\n",
        "        attention_scores = dict()\n",
        "\n",
        "        for transformer_module in self.transformer_blocks:\n",
        "            encoded_patches, attention_score = transformer_module(encoded_patches)\n",
        "            attention_scores[f\"{transformer_module.name}_att\"] = attention_score\n",
        "\n",
        "        representation = self.layer_norm(encoded_patches)\n",
        "\n",
        "        encoded_patches = representation[:, 0]\n",
        "        encoded_patches_exp = tf.expand_dims(encoded_patches, -1)\n",
        "        avg_patch_tokens = tf.reduce_mean(representation[:, 1:], 1)\n",
        "        avg_patch_tokens = tf.expand_dims(avg_patch_tokens, -1)\n",
        "        output = tf.concat([encoded_patches_exp, avg_patch_tokens], -1)\n",
        "        output = tf.reshape(output, (n, -1))\n",
        "\n",
        "        # Classification head.\n",
        "        output = self.head(output)\n",
        "\n",
        "        if training:\n",
        "            return output\n",
        "        else:\n",
        "            return output, attention_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGLFef4qfJSx",
        "outputId": "afcccc05-82ad-402a-8466-b194db94448a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2, 12, 197, 197)\n"
          ]
        }
      ],
      "source": [
        "config = base_config.get_config(model_name=\"vit_base\", projection_dim=768, num_heads=12)\n",
        "\n",
        "vit_dino_base = ViTDINOBase(config)\n",
        "\n",
        "dummy_inputs = tf.random.normal((2, 224, 224, 3))\n",
        "outputs, attn_scores = vit_dino_base(dummy_inputs)\n",
        "\n",
        "keys = list(attn_scores.keys())\n",
        "print(attn_scores[keys[-1]].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "k0Tw_vLTfMlz"
      },
      "outputs": [],
      "source": [
        "pt_model_dict = {k: backbone_state_dict[k].numpy() for k in backbone_state_dict}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dQdDkHt5fOmb"
      },
      "outputs": [],
      "source": [
        "vit_dino_base.layers[0].layers[0] = helpers.modify_tf_block(\n",
        "    vit_dino_base.layers[0].layers[0],\n",
        "    pt_model_dict[\"patch_embed.proj.weight\"],\n",
        "    pt_model_dict[\"patch_embed.proj.bias\"],\n",
        ")\n",
        "\n",
        "# Positional embedding.\n",
        "vit_dino_base.positional_embedding.assign(tf.Variable(pt_model_dict[\"pos_embed\"]))\n",
        "vit_dino_base.cls_token.assign(tf.Variable(pt_model_dict[\"cls_token\"]))\n",
        "\n",
        "# Layer norm layers.\n",
        "ln_idx = -2\n",
        "vit_dino_base.layers[ln_idx] = helpers.modify_tf_block(\n",
        "    vit_dino_base.layers[ln_idx],\n",
        "    pt_model_dict[\"norm.weight\"],\n",
        "    pt_model_dict[\"norm.bias\"],\n",
        ")\n",
        "\n",
        "# Head layers.\n",
        "head_layer = vit_dino_base.get_layer(\"classification_head\")\n",
        "vit_dino_base.layers[-1] = helpers.modify_tf_block(\n",
        "    head_layer,\n",
        "    pt_model_dict[\"head.weight\"],\n",
        "    pt_model_dict[\"head.bias\"],\n",
        ")\n",
        "\n",
        "# Transformer blocks.\n",
        "idx = 0\n",
        "\n",
        "for outer_layer in vit_dino_base.layers:\n",
        "    if isinstance(outer_layer, tf.keras.Model) and outer_layer.name != \"projection\":\n",
        "        tf_block = vit_dino_base.get_layer(outer_layer.name)\n",
        "        pt_block_name = f\"blocks.{idx}\"\n",
        "\n",
        "        # LayerNorm layers.\n",
        "        layer_norm_idx = 1\n",
        "        for layer in tf_block.layers:\n",
        "            if isinstance(layer, tf.keras.layers.LayerNormalization):\n",
        "                layer_norm_pt_prefix = f\"{pt_block_name}.norm{layer_norm_idx}\"\n",
        "                layer.gamma.assign(\n",
        "                    tf.Variable(pt_model_dict[f\"{layer_norm_pt_prefix}.weight\"])\n",
        "                )\n",
        "                layer.beta.assign(\n",
        "                    tf.Variable(pt_model_dict[f\"{layer_norm_pt_prefix}.bias\"])\n",
        "                )\n",
        "                layer_norm_idx += 1\n",
        "\n",
        "        # FFN layers.\n",
        "        ffn_layer_idx = 1\n",
        "        for layer in tf_block.layers:\n",
        "            if isinstance(layer, tf.keras.layers.Dense):\n",
        "                dense_layer_pt_prefix = f\"{pt_block_name}.mlp.fc{ffn_layer_idx}\"\n",
        "                layer = helpers.modify_tf_block(\n",
        "                    layer,\n",
        "                    pt_model_dict[f\"{dense_layer_pt_prefix}.weight\"],\n",
        "                    pt_model_dict[f\"{dense_layer_pt_prefix}.bias\"],\n",
        "                )\n",
        "                ffn_layer_idx += 1\n",
        "\n",
        "        # Attention layer.\n",
        "        for layer in tf_block.layers:\n",
        "            (q_w, k_w, v_w), (q_b, k_b, v_b) = helpers.get_tf_qkv(\n",
        "                f\"{pt_block_name}.attn\",\n",
        "                pt_model_dict,\n",
        "                config,\n",
        "            )\n",
        "\n",
        "            if isinstance(layer, mha.TFViTAttention):\n",
        "                # Key\n",
        "                layer.self_attention.key = helpers.modify_tf_block(\n",
        "                    layer.self_attention.key,\n",
        "                    k_w,\n",
        "                    k_b,\n",
        "                    is_attn=True,\n",
        "                )\n",
        "                # Query\n",
        "                layer.self_attention.query = helpers.modify_tf_block(\n",
        "                    layer.self_attention.query,\n",
        "                    q_w,\n",
        "                    q_b,\n",
        "                    is_attn=True,\n",
        "                )\n",
        "                # Value\n",
        "                layer.self_attention.value = helpers.modify_tf_block(\n",
        "                    layer.self_attention.value,\n",
        "                    v_w,\n",
        "                    v_b,\n",
        "                    is_attn=True,\n",
        "                )\n",
        "                # Final dense projection\n",
        "                layer.dense_output.dense = helpers.modify_tf_block(\n",
        "                    layer.dense_output.dense,\n",
        "                    pt_model_dict[f\"{pt_block_name}.attn.proj.weight\"],\n",
        "                    pt_model_dict[f\"{pt_block_name}.attn.proj.bias\"],\n",
        "                )\n",
        "\n",
        "        idx += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dno-7td8fVLa"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import cv2\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GKAxZ5vufXdH"
      },
      "outputs": [],
      "source": [
        "FOURCC = {\n",
        "    \"mp4\": cv2.VideoWriter_fourcc(*\"MP4V\"),\n",
        "    \"avi\": cv2.VideoWriter_fourcc(*\"XVID\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4BV5rBHBfY-a"
      },
      "outputs": [],
      "source": [
        "class VideoGeneratorTF:\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "\n",
        "        self.norm_layer = keras.layers.Normalization(\n",
        "            mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
        "            variance=[(0.229 * 255) ** 2, (0.224 * 255) ** 2, (0.225 * 255) ** 2],\n",
        "        )\n",
        "\n",
        "    def run(self):\n",
        "        if self.args.input_path is None:\n",
        "            print(f\"Provided input path {self.args.input_path} is non valid.\")\n",
        "            sys.exit(1)\n",
        "        else:\n",
        "            if self.args.video_only:\n",
        "                self._generate_video_from_images(\n",
        "                    self.args.input_path, self.args.output_path\n",
        "                )\n",
        "            else:\n",
        "                # If input path exists\n",
        "                if os.path.exists(self.args.input_path):\n",
        "                    # If input is a video file\n",
        "                    if os.path.isfile(self.args.input_path):\n",
        "                        frames_folder = os.path.join(self.args.output_path, \"frames-tf\")\n",
        "                        attention_folder = os.path.join(\n",
        "                            self.args.output_path, \"attention-tf\"\n",
        "                        )\n",
        "\n",
        "                        os.makedirs(frames_folder, exist_ok=True)\n",
        "                        os.makedirs(attention_folder, exist_ok=True)\n",
        "\n",
        "                        self._extract_frames_from_video(\n",
        "                            self.args.input_path, frames_folder\n",
        "                        )\n",
        "\n",
        "                        self._inference(\n",
        "                            frames_folder,\n",
        "                            attention_folder,\n",
        "                        )\n",
        "\n",
        "                        self._generate_video_from_images(\n",
        "                            attention_folder, self.args.output_path\n",
        "                        )\n",
        "\n",
        "                    # If input is a folder of already extracted frames\n",
        "                    if os.path.isdir(self.args.input_path):\n",
        "                        attention_folder = os.path.join(\n",
        "                            self.args.output_path, \"attention-tf\"\n",
        "                        )\n",
        "\n",
        "                        os.makedirs(attention_folder, exist_ok=True)\n",
        "\n",
        "                        self._inference(self.args.input_path, attention_folder)\n",
        "\n",
        "                        self._generate_video_from_images(\n",
        "                            attention_folder, self.args.output_path\n",
        "                        )\n",
        "\n",
        "                # If input path doesn't exists\n",
        "                else:\n",
        "                    print(f\"Provided input path {self.args.input_path} doesn't exists.\")\n",
        "                    sys.exit(1)\n",
        "\n",
        "    def _extract_frames_from_video(self, inp: str, out: str):\n",
        "        vidcap = cv2.VideoCapture(inp)\n",
        "        self.args.fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "        print(f\"Video: {inp} ({self.args.fps} fps)\")\n",
        "        print(f\"Extracting frames to {out}\")\n",
        "\n",
        "        success, image = vidcap.read()\n",
        "        count = 0\n",
        "        while success:\n",
        "            cv2.imwrite(\n",
        "                os.path.join(out, f\"frame-{count:04}.jpg\"),\n",
        "                image,\n",
        "            )\n",
        "            success, image = vidcap.read()\n",
        "            count += 1\n",
        "\n",
        "    def _generate_video_from_images(self, inp: str, out: str):\n",
        "        img_array = []\n",
        "        attention_images_list = sorted(glob.glob(os.path.join(inp, \"attn-*.jpg\")))\n",
        "\n",
        "        # Get size of the first image\n",
        "        with open(attention_images_list[0], \"rb\") as f:\n",
        "            img = Image.open(f)\n",
        "            img = img.convert(\"RGB\")\n",
        "            size = (img.width, img.height)\n",
        "            img_array.append(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        print(f\"Generating video {size} to {out}\")\n",
        "\n",
        "        for filename in tqdm(attention_images_list[1:]):\n",
        "            with open(filename, \"rb\") as f:\n",
        "                img = Image.open(f)\n",
        "                img = img.convert(\"RGB\")\n",
        "                img_array.append(cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        out = cv2.VideoWriter(\n",
        "            os.path.join(out, \"video-tf.\" + self.args.video_format),\n",
        "            FOURCC[self.args.video_format],\n",
        "            self.args.fps,\n",
        "            size,\n",
        "        )\n",
        "\n",
        "        for i in range(len(img_array)):\n",
        "            out.write(img_array[i])\n",
        "        out.release()\n",
        "        print(\"Done\")\n",
        "\n",
        "    def _preprocess_image(self, image: Image, size: int):\n",
        "        image = np.array(image)\n",
        "        image_resized = tf.expand_dims(image, 0)\n",
        "        shape = tf.cast(tf.shape(image_resized)[1:-1], tf.float32)\n",
        "        short_dim = min(shape)\n",
        "        scale = size / short_dim\n",
        "        new_shape = tf.cast(shape * scale, tf.int32)\n",
        "        image_resized = tf.image.resize(\n",
        "            image_resized,\n",
        "            new_shape,\n",
        "        )\n",
        "        return self.norm_layer(image_resized).numpy()\n",
        "\n",
        "    def _inference(self, inp: str, out: str):\n",
        "        print(f\"Generating attention images to {out}\")\n",
        "\n",
        "        for img_path in tqdm(sorted(glob.glob(os.path.join(inp, \"*.jpg\")))):\n",
        "            with open(img_path, \"rb\") as f:\n",
        "                img = Image.open(f)\n",
        "                img = img.convert(\"RGB\")\n",
        "\n",
        "            preprocessed_image = self._preprocess_image(img, self.args.resize)\n",
        "            h, w = (\n",
        "                preprocessed_image.shape[1]\n",
        "                - preprocessed_image.shape[1] % self.args.patch_size,\n",
        "                preprocessed_image.shape[2]\n",
        "                - preprocessed_image.shape[2] % self.args.patch_size,\n",
        "            )\n",
        "            preprocessed_image = preprocessed_image[:, :h, :w, :]\n",
        "\n",
        "            h_featmap = preprocessed_image.shape[1] // self.args.patch_size\n",
        "            w_featmap = preprocessed_image.shape[2] // self.args.patch_size\n",
        "\n",
        "            logits, attention_score_dict = self.args.model(\n",
        "                preprocessed_image, training=False\n",
        "            )\n",
        "            attentions = attention_score_dict[\"transformer_block_11_att\"].numpy()\n",
        "\n",
        "            nh = attentions.shape[1]  # number of head\n",
        "\n",
        "            # we keep only the output patch attention\n",
        "            attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
        "            attentions = attentions.reshape(nh, h_featmap, w_featmap)\n",
        "            attentions = attentions.transpose((1, 2, 0))\n",
        "\n",
        "            # interpolation\n",
        "            attentions = tf.image.resize(\n",
        "                attentions,\n",
        "                size=(\n",
        "                    h_featmap * self.args.patch_size,\n",
        "                    w_featmap * self.args.patch_size,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            # save attentions heatmaps\n",
        "            fname = os.path.join(out, \"attn-\" + os.path.basename(img_path))\n",
        "            plt.imsave(\n",
        "                fname=fname,\n",
        "                arr=sum(\n",
        "                    attentions[..., i] * 1 / attentions.shape[-1]\n",
        "                    for i in range(attentions.shape[-1])\n",
        "                ),\n",
        "                cmap=\"inferno\",\n",
        "                format=\"jpg\",\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHah6q4vE9ds"
      },
      "source": [
        "## Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1GJTgMYfdZs",
        "outputId": "4fd0c2ab-ebe5-46a7-d6c0-e515a54c831a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12KScLSdZS5gNvLqoZBenbYeTPaVx4wMj\n",
            "To: /content/dog.mp4\n",
            "100% 12.8M/12.8M [00:00<00:00, 156MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dnPP0QvJ2944GaSE47yMgrt3T0yO4R_R\n",
            "To: /content/dino.mp4\n",
            "100% 2.18M/2.18M [00:00<00:00, 178MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Get demo videos.\n",
        "!gdown --id 12KScLSdZS5gNvLqoZBenbYeTPaVx4wMj\n",
        "!gdown --id 1dnPP0QvJ2944GaSE47yMgrt3T0yO4R_R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "x3xhuN6Hfto8"
      },
      "outputs": [],
      "source": [
        "args = ml_collections.ConfigDict()\n",
        "\n",
        "args.model = vit_dino_base\n",
        "args.patch_size = 16\n",
        "args.input_path = \"dino.mp4\"\n",
        "args.output_path = \"./\"\n",
        "args.resize = 512\n",
        "args.video_only = False\n",
        "args.fps = 30.0\n",
        "args.video_format = \"mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME1rBiRifyfS",
        "outputId": "718fd00f-9215-4886-a4b4-05497e7e456d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video: dino.mp4 (25.0 fps)\n",
            "Extracting frames to ./frames-tf\n",
            "Generating attention images to ./attention-tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 278/278 [01:23<00:00,  3.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating video (896, 512) to ./\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 277/277 [00:01<00:00, 157.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "vg = VideoGeneratorTF(args)\n",
        "vg.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cCAvESwJf0pb"
      },
      "outputs": [],
      "source": [
        "args = ml_collections.ConfigDict()\n",
        "\n",
        "args.model = vit_dino_base\n",
        "args.patch_size = 16\n",
        "args.input_path = \"dog.mp4\"\n",
        "args.output_path = \"./\"\n",
        "args.resize = 512\n",
        "args.video_only = False\n",
        "args.fps = 30.0\n",
        "args.video_format = \"mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQ8ZQdE7guzq",
        "outputId": "2f82bec1-f6c3-488a-89c0-544be73fb9b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video: dog.mp4 (29.97002997002997 fps)\n",
            "Extracting frames to ./frames-tf\n",
            "Generating attention images to ./attention-tf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [01:16<00:00,  1.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating video (896, 512) to ./\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 149/149 [00:01<00:00, 135.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "vg = VideoGeneratorTF(args)\n",
        "vg.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYPkwMCagxLO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ExplainViT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
